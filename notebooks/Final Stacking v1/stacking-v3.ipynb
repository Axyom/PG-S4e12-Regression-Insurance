{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Define the base directory\n",
    "base_dir = Path.cwd().resolve().parents[1]\n",
    "\n",
    "# Define subdirectories\n",
    "data_dir = base_dir / \"data\"\n",
    "model_dir = base_dir / \"models\"\n",
    "notebooks_dir = base_dir / \"notebooks\"\n",
    "\n",
    "# Append base_dir to sys.path\n",
    "sys.path.append(str(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeta-test shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_meta_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_meta_train, X_meta_test\n\u001b[1;32m---> 37\u001b[0m X_meta_train, X_meta_test \u001b[38;5;241m=\u001b[39m\u001b[43mget_meta_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnotebooks/Ensemble Catboost XGBoost LGBM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36mget_meta_data\u001b[1;34m(meta_data_dir)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Load OOF and test predictions with identical features\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m X_meta_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43moof_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOOF_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m X_meta_test \u001b[38;5;241m=\u001b[39m load_and_prefix(test_files, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Ensure the columns are identical between train and test\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mget_meta_data.<locals>.load_and_prefix\u001b[1;34m(files, prefix)\u001b[0m\n\u001b[0;32m     20\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_F\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[0;32m     21\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Axyom\\anaconda3\\envs\\kaggle_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Axyom\\anaconda3\\envs\\kaggle_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Users\\Axyom\\anaconda3\\envs\\kaggle_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def get_meta_data(meta_data_dir):\n",
    "    meta_data_dir = Path(meta_data_dir)\n",
    "    # Define file patterns for OOF and test predictions\n",
    "    oof_files = sorted(glob.glob(str(meta_data_dir/\"*_oof.csv\")))  # Sorted for consistency\n",
    "    test_files = sorted(glob.glob(str(meta_data_dir/\"*_test.csv\")))  # Sorted for consistency\n",
    "\n",
    "    # Ensure the number of files matches\n",
    "    assert len(oof_files) == len(test_files), \"Mismatch in the number of OOF and test files.\"\n",
    "\n",
    "    # Function to load and rename files with prefixes\n",
    "    def load_and_prefix(files, prefix):\n",
    "        dfs = []\n",
    "        for file in files:\n",
    "            model_name = file.split(\"/\")[-1].replace(\"_oof.csv\", \"\").replace(\"_test.csv\", \"\")\n",
    "            df = pd.read_csv(file)\n",
    "            df.columns = [f\"{prefix}{model_name}_F{i+1}\" for i in range(df.shape[1])]\n",
    "            dfs.append(df)\n",
    "        return pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Load OOF and test predictions with identical features\n",
    "    X_meta_train = load_and_prefix(oof_files, prefix=\"OOF_\")\n",
    "    X_meta_test = load_and_prefix(test_files, prefix=\"Test_\")\n",
    "\n",
    "    # Ensure the columns are identical between train and test\n",
    "    X_meta_test.columns = X_meta_train.columns\n",
    "\n",
    "    # Check the shapes\n",
    "    print(f\"Meta-train shape: {X_meta_train.shape}\")\n",
    "    print(f\"Meta-test shape: {X_meta_test.shape}\")\n",
    "    \n",
    "    return X_meta_train, X_meta_test\n",
    "\n",
    "X_meta_train, X_meta_test =get_meta_data(\"notebooks/Ensemble Catboost XGBoost LGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T07:30:09.821020Z",
     "iopub.status.busy": "2024-12-25T07:30:09.820547Z",
     "iopub.status.idle": "2024-12-25T07:30:10.257246Z",
     "shell.execute_reply": "2024-12-25T07:30:10.256021Z",
     "shell.execute_reply.started": "2024-12-25T07:30:09.820988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Mock\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a mock meta_features DataFrame (3 models, 100 samples)\n",
    "X_meta_train = pd.DataFrame({\n",
    "    \"model_1\": np.random.rand(100) * 0.8 + 0.1,  # Predictions from model 1\n",
    "    \"model_2\": np.random.rand(100) * 0.9 + 0.05, # Predictions from model 2\n",
    "    \"model_3\": np.random.rand(100) * 0.85 + 0.1  # Predictions from model 3\n",
    "})\n",
    "\n",
    "# Create a mock meta_predictions DataFrame for the test set (3 models, 50 samples)\n",
    "X_meta_test = pd.DataFrame({\n",
    "    \"model_1\": np.random.rand(50) * 0.8 + 0.1,\n",
    "    \"model_2\": np.random.rand(50) * 0.9 + 0.05,\n",
    "    \"model_3\": np.random.rand(50) * 0.85 + 0.1\n",
    "})\n",
    "\n",
    "# Create a mock y_train Series (true target values)\n",
    "y_train = pd.Series(np.random.rand(100) * 0.9 + 0.05)\n",
    "\n",
    "# Print the shapes and samples\n",
    "print(\"meta_features (train):\")\n",
    "print(X_meta_train.head())\n",
    "print(\"\\nmeta_predictions (test):\")\n",
    "print(X_meta_test.head())\n",
    "print(\"\\ny_train (target):\")\n",
    "print(y_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T07:30:10.259599Z",
     "iopub.status.busy": "2024-12-25T07:30:10.258931Z",
     "iopub.status.idle": "2024-12-25T07:30:12.383366Z",
     "shell.execute_reply": "2024-12-25T07:30:12.382272Z",
     "shell.execute_reply.started": "2024-12-25T07:30:10.259566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from optuna.visualization import plot_optimization_history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def optimize_ensemble_weights(X_meta_train, X_meta_test, y_train, n_trials=100):\n",
    "    \"\"\"\n",
    "    Optimize ensemble weights for a given set of meta-features using Optuna.\n",
    "    \n",
    "    Parameters:\n",
    "        X_meta_train (pd.DataFrame): OOF predictions (meta-features) for training.\n",
    "        X_meta_test (pd.DataFrame): Predictions for the test set (meta-features).\n",
    "        y_train (pd.Series): True target values for training.\n",
    "        n_trials (int): Number of trials for Optuna optimization.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Optimized weights for each meta-model.\n",
    "        np.ndarray: Final predictions on the test set.\n",
    "    \"\"\"\n",
    "    # Define the objective function\n",
    "    def objective(trial):\n",
    "        num_models = X_meta_train.shape[1]\n",
    "        weights = [trial.suggest_float(f\"weight_{i}\", 0, 1) for i in range(num_models)]\n",
    "        weights = np.array(weights) / sum(weights)  # Normalize weights to sum to 1\n",
    "        ensemble_preds = (X_meta_train.values * weights).sum(axis=1)\n",
    "        rmse = np.sqrt(mean_squared_error(y_train, ensemble_preds))\n",
    "        return rmse\n",
    "\n",
    "    # Initialize and optimize the study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # df = study.trials_dataframe()\n",
    "    # print(df)  \n",
    "\n",
    "    # Retrieve the best weights\n",
    "    best_weights = study.best_params\n",
    "    normalized_weights = np.array([best_weights[f\"weight_{i}\"] for i in range(len(best_weights))])\n",
    "    normalized_weights /= normalized_weights.sum()\n",
    "\n",
    "    # Generate predictions for the test set\n",
    "    test_preds = (X_meta_test.values * normalized_weights).sum(axis=1)\n",
    "\n",
    "    # Print optimization results\n",
    "    print(\"Optimized Weights:\", normalized_weights)\n",
    "    print(\"Best RMSE:\", study.best_value)\n",
    "\n",
    "    # Plot optimization history\n",
    "    plot = plot_optimization_history(study)\n",
    "    plt.title(\"Optuna Optimization History\")\n",
    "    plt.show()\n",
    "\n",
    "    return best_weights, test_preds\n",
    "\n",
    "# Assuming meta_features (training meta-features), meta_predictions (test meta-features),\n",
    "# and y_train (true target values) are already defined.\n",
    "optimized_weights, test_predictions = optimize_ensemble_weights(\n",
    "    X_meta_train=X_meta_train,\n",
    "    X_meta_test=X_meta_test,\n",
    "    y_train=y_train,\n",
    "    n_trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T07:31:10.866415Z",
     "iopub.status.busy": "2024-12-25T07:31:10.865996Z",
     "iopub.status.idle": "2024-12-25T07:31:13.599209Z",
     "shell.execute_reply": "2024-12-25T07:31:13.597777Z",
     "shell.execute_reply.started": "2024-12-25T07:31:10.866389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from axyom_utilities.training import train_model_cv\n",
    "\n",
    "# Neural Network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, learning_rate=0.001):\n",
    "        self.model = Sequential([\n",
    "            Dense(128, activation='relu', input_dim=input_dim),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        self.model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs=50, batch_size=32, verbose=0):\n",
    "        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X).flatten()\n",
    "\n",
    "# Initialize models\n",
    "meta_models = {\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=42, verbose=-1),\n",
    "    \"NeuralNetwork\": NeuralNetwork(input_dim=X_meta_train.shape[1])\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in meta_models.items():\n",
    "    results = train_model_cv(model, X_meta_train, y_train, X_meta_test, cv_splits=10)\n",
    "    \n",
    "    oof_preds = results['oof_preds']\n",
    "    test_preds = results['test_preds']\n",
    "    score = np.mean(results['cv_scores'])\n",
    "    \n",
    "    results[name] = {\"oof_preds\": oof_preds, \"test_preds\": test_preds, \"score\":score}\n",
    "    print(f\"{name} OOF RMSE: {score:.6f}\")\n",
    "\n",
    "# Compare stacking methods\n",
    "# for method, result in results.items():\n",
    "#     print(f\"{method}: OOF RMSE = {result['oof_rmse']:.4f}\")\n",
    "# Find the best model\n",
    "best_model = max(results.items(), key=lambda x: x[1][\"score\"])\n",
    "best_name, best_data = best_model\n",
    "\n",
    "# Extract relevant details\n",
    "best_oof_preds = best_data[\"oof_preds\"]\n",
    "best_test_preds = best_data[\"test_preds\"]\n",
    "\n",
    "print(f\"Best Model Name: {best_name}\")\n",
    "print(f\"OOF Predictions: {best_oof_preds}\")\n",
    "print(f\"Test Predictions: {best_test_preds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T07:31:19.684885Z",
     "iopub.status.busy": "2024-12-25T07:31:19.684538Z",
     "iopub.status.idle": "2024-12-25T07:31:19.712951Z",
     "shell.execute_reply": "2024-12-25T07:31:19.711436Z",
     "shell.execute_reply.started": "2024-12-25T07:31:19.684860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.maximum(0, np.expm1(test_preds))\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': X_meta_test.index,  \n",
    "    'Premium Amount': y_pred\n",
    "})\n",
    "\n",
    "FILE_PATH = f\"Stacking_v3_{best_model_score:.4f}.csv\"\n",
    "\n",
    "submission.to_csv(FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10305135,
     "sourceId": 84896,
     "sourceType": "competition"
    },
    {
     "sourceId": 214389003,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
